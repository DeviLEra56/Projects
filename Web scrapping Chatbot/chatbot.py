# -*- coding: utf-8 -*-
"""chatbot.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19IRdAaXQ8s3RZdWfFnBIb63dTRUPp5vN
"""

#Setting up the environment

import requests
from bs4 import BeautifulSoup
import openai
import nltk #Import NLTK ---> Natural Language Toolkit
from nltk.tokenize import sent_tokenize
nltk.download('punkt')

# Set up your OpenAI API key
openai.api_key = "your API Key"





# Extracting data
# Function to scrape a website and extract relevant information
def scrape_website(url):
    try:
        response = requests.get(url)
        if response.status_code == 200:
            soup = BeautifulSoup(response.content, 'html.parser')

            extracted_info = " ".join([p.get_text() for p in soup.find_all('p')])



            return extracted_info
        else:
            return None
    except Exception as e:
        print("Error scraping the website:", e)
        return None






#Proessing Data

def cleaned_data(infoo):
           #website_text = "\n".join(infoo)

           website_text = re.sub(r'[^\w\s]()', '', infoo)
           sentences = sent_tokenize(website_text)
           return sentences




# Function to interact with the chatbot


def chat_with_bot(web_infoo, your_input):
    user_prompt = f"User: {your_input}\nWebsite: {web_infoo}\nBot:"
    response = openai.Completion.create(
        engine="davinci",
        prompt=user_prompt,
        max_tokens=50
    )
    return response.choices[0].text

if __name__ == "__main__":
    website_url = input("Enter the URL of the website: ")
    website_info = scrape_website(website_url)
    web_infoo=cleaned_data(website_info)

    if website_info:
        print("Website information scraped successfully.")
        while True:
            your_input = input("You: ")
            if your_input.lower() == "exit":
                break
            bot_response = chat_with_bot(web_infoo, your_input)
            print("Bot:", bot_response)
    else:
        print("Failed to scrape the website. Please check the URL.")